{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Segformer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.20) or chardet (5.2.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "from huggingface_hub import hf_hub_download\n",
    "import json\n",
    "import torchvision.transforms as t\n",
    "from transformers import AutoImageProcessor\n",
    "from torchvision.transforms import ColorJitter\n",
    "from transformers import SegformerImageProcessor\n",
    "from torchvision.transforms import v2\n",
    "import torch\n",
    "from torch import nn\n",
    "import evaluate\n",
    "from transformers import Trainer\n",
    "from transformers import SegformerForSemanticSegmentation\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access token to get access to the dataset. \n",
    "\n",
    "You just need a HuggingFace account to have access to it. \n",
    "\n",
    "If you don't have a HuggingFace account use the temporary token shared in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_token = \"place_token_in_this string\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since segments/sidewalk-semantic couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/segments___sidewalk-semantic/default/0.0.0/9c33630210cfdc58ab3680f425d44b79c4d03c53 (last modified on Wed Feb 19 03:35:10 2025).\n"
     ]
    }
   ],
   "source": [
    "hf_dataset_identifier = \"segments/sidewalk-semantic\"\n",
    "ds = load_dataset(hf_dataset_identifier,\n",
    "                   token=access_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare labels :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"id2label.json\"\n",
    "id2label = json.load(\n",
    "    open(hf_hub_download(hf_dataset_identifier, filename, repo_type=\"dataset\",token=access_token), \"r\")\n",
    ")\n",
    "id2label = {int(k): v for k, v in id2label.items()}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "num_labels = len(id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35,\n",
       " ['unlabeled',\n",
       "  'flat-road',\n",
       "  'flat-sidewalk',\n",
       "  'flat-crosswalk',\n",
       "  'flat-cyclinglane',\n",
       "  'flat-parkingdriveway',\n",
       "  'flat-railtrack',\n",
       "  'flat-curb',\n",
       "  'human-person',\n",
       "  'human-rider',\n",
       "  'vehicle-car',\n",
       "  'vehicle-truck',\n",
       "  'vehicle-bus',\n",
       "  'vehicle-tramtrain',\n",
       "  'vehicle-motorcycle',\n",
       "  'vehicle-bicycle',\n",
       "  'vehicle-caravan',\n",
       "  'vehicle-cartrailer',\n",
       "  'construction-building',\n",
       "  'construction-door',\n",
       "  'construction-wall',\n",
       "  'construction-fenceguardrail',\n",
       "  'construction-bridge',\n",
       "  'construction-tunnel',\n",
       "  'construction-stairs',\n",
       "  'object-pole',\n",
       "  'object-trafficsign',\n",
       "  'object-trafficlight',\n",
       "  'nature-vegetation',\n",
       "  'nature-terrain',\n",
       "  'sky',\n",
       "  'void-ground',\n",
       "  'void-dynamic',\n",
       "  'void-static',\n",
       "  'void-unclear'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_labels, list(label2id.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the preprocessing for the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SegformerImageProcessor {\n",
       "  \"do_normalize\": true,\n",
       "  \"do_reduce_labels\": false,\n",
       "  \"do_rescale\": true,\n",
       "  \"do_resize\": true,\n",
       "  \"image_mean\": [\n",
       "    0.485,\n",
       "    0.456,\n",
       "    0.406\n",
       "  ],\n",
       "  \"image_processor_type\": \"SegformerImageProcessor\",\n",
       "  \"image_std\": [\n",
       "    0.229,\n",
       "    0.224,\n",
       "    0.225\n",
       "  ],\n",
       "  \"resample\": 2,\n",
       "  \"rescale_factor\": 0.00392156862745098,\n",
       "  \"size\": {\n",
       "    \"height\": 512,\n",
       "    \"width\": 512\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model_name = \"nvidia/mit-b2\" \n",
    "image_processor = AutoImageProcessor.from_pretrained(pretrained_model_name)\n",
    "image_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def transforms(image):\n",
    "    image = t.ToTensor(image)\n",
    "    image = image.permute(\n",
    "        (2, 0, 1)\n",
    "    )  # because model is channels-first\n",
    "    return image\n",
    "\n",
    "\n",
    "def preprocess(example_batch):\n",
    "    images = [transforms(x.convert(\"RGB\")) for x in example_batch[\"pixel_values\"]]\n",
    "    labels = [x for x in example_batch[\"label\"]]\n",
    "    inputs = image_processor(images, labels)\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "processor = SegformerImageProcessor()\n",
    "\n",
    "image_only_transforms = v2.Compose([\n",
    "     ColorJitter(brightness=0.25, contrast=0.25, saturation=0.25, hue=0.1),\n",
    "])\n",
    "label_image_transforms = v2.Compose([\n",
    "    v2.RandomHorizontalFlip(p=0.5),\n",
    "    v2.RandomVerticalFlip(p=0.5),\n",
    "])\n",
    "\n",
    "def train_transforms(example_batch):\n",
    "    images = [image_only_transforms(x) for x in example_batch['pixel_values']]\n",
    "    labels = [x for x in example_batch['label']]\n",
    "\n",
    "    images, labels= label_image_transforms(images, labels)\n",
    "    inputs = processor(images, labels)\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def val_transforms(example_batch):\n",
    "    images = [x for x in example_batch['pixel_values']]\n",
    "    labels = [x for x in example_batch['label']]\n",
    "    inputs = processor(images, labels)\n",
    "    return inputs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split up training into training + validation\n",
    "splits = ds[\"train\"].train_test_split(test_size=0.2)\n",
    "train_ds = splits[\"train\"]\n",
    "val_ds = splits[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set transforms\n",
    "train_ds.set_transform(train_transforms)\n",
    "val_ds.set_transform(val_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load our segformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/mit-b2 and are newly initialized: ['decode_head.batch_norm.bias', 'decode_head.batch_norm.num_batches_tracked', 'decode_head.batch_norm.running_mean', 'decode_head.batch_norm.running_var', 'decode_head.batch_norm.weight', 'decode_head.classifier.bias', 'decode_head.classifier.weight', 'decode_head.linear_c.0.proj.bias', 'decode_head.linear_c.0.proj.weight', 'decode_head.linear_c.1.proj.bias', 'decode_head.linear_c.1.proj.weight', 'decode_head.linear_c.2.proj.bias', 'decode_head.linear_c.2.proj.weight', 'decode_head.linear_c.3.proj.bias', 'decode_head.linear_c.3.proj.weight', 'decode_head.linear_fuse.weight', 'decode_head.wide_segment.0.bias', 'decode_head.wide_segment.0.weight', 'decode_head.wide_segment.1.bias', 'decode_head.wide_segment.1.num_batches_tracked', 'decode_head.wide_segment.1.running_mean', 'decode_head.wide_segment.1.running_var', 'decode_head.wide_segment.1.weight', 'decode_head.wide_segment.12.bias', 'decode_head.wide_segment.12.weight', 'decode_head.wide_segment.13.bias', 'decode_head.wide_segment.13.num_batches_tracked', 'decode_head.wide_segment.13.running_mean', 'decode_head.wide_segment.13.running_var', 'decode_head.wide_segment.13.weight', 'decode_head.wide_segment.4.bias', 'decode_head.wide_segment.4.weight', 'decode_head.wide_segment.5.bias', 'decode_head.wide_segment.5.num_batches_tracked', 'decode_head.wide_segment.5.running_mean', 'decode_head.wide_segment.5.running_var', 'decode_head.wide_segment.5.weight', 'decode_head.wide_segment.8.bias', 'decode_head.wide_segment.8.weight', 'decode_head.wide_segment.9.bias', 'decode_head.wide_segment.9.num_batches_tracked', 'decode_head.wide_segment.9.running_mean', 'decode_head.wide_segment.9.running_var', 'decode_head.wide_segment.9.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#use the mit-b2 encoder for our experiments\n",
    "pretrained_model_name = \"nvidia/mit-b2\" \n",
    "model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    pretrained_model_name,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    long_decoder_depth=0,\n",
    "    width_list=[1024, 1280, 1536, 1792]\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SegformerDecodeHead(\n",
      "  (linear_c): ModuleList(\n",
      "    (0): SegformerMLP(\n",
      "      (proj): Linear(in_features=64, out_features=768, bias=True)\n",
      "    )\n",
      "    (1): SegformerMLP(\n",
      "      (proj): Linear(in_features=128, out_features=768, bias=True)\n",
      "    )\n",
      "    (2): SegformerMLP(\n",
      "      (proj): Linear(in_features=320, out_features=768, bias=True)\n",
      "    )\n",
      "    (3): SegformerMLP(\n",
      "      (proj): Linear(in_features=512, out_features=768, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (linear_fuse): Conv2d(3072, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (batch_norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (activation): ReLU()\n",
      "  (wide_segment): Sequential(\n",
      "    (0): Conv2d(768, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.1, inplace=False)\n",
      "    (4): Conv2d(1024, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (5): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): Dropout(p=0.1, inplace=False)\n",
      "    (8): Conv2d(1280, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (9): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU()\n",
      "    (11): Dropout(p=0.1, inplace=False)\n",
      "    (12): Conv2d(1536, 1792, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (13): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (14): ReLU()\n",
      "    (15): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Conv2d(1792, 35, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model.decode_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10045731\n"
     ]
    }
   ],
   "source": [
    "print(model.decode_head.num_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set the parameters for the trainning pipeline :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#set the parameters for the trainning \n",
    "epochs = 80\n",
    "lr = 0.00006\n",
    "batch_size = 4\n",
    "checkpoints_path = \"./training_logs\"\n",
    "training_args = TrainingArguments(\n",
    "    checkpoints_path,\n",
    "    learning_rate=lr,\n",
    "    num_train_epochs=epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    save_total_limit=3,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=20,\n",
    "    eval_steps=20,\n",
    "    logging_steps=1,\n",
    "    eval_accumulation_steps=5,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_mean_iou\",\n",
    "    greater_is_better = True,\n",
    "    lr_scheduler_type=\"polynomial\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metric for the trainning and the evaluation (loss and mIOU):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "metric = evaluate.load(\"mean_iou\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "  with torch.no_grad():\n",
    "    logits, labels = eval_pred\n",
    "    logits_tensor = torch.from_numpy(logits)\n",
    "    # scale the logits to the size of the label\n",
    "    logits_tensor = nn.functional.interpolate(\n",
    "        logits_tensor,\n",
    "        size=labels.shape[-2:],\n",
    "        mode=\"bilinear\",\n",
    "        align_corners=False,\n",
    "    ).argmax(dim=1)\n",
    "\n",
    "    pred_labels = logits_tensor.detach().cpu().numpy()\n",
    "    metrics = metric.compute(\n",
    "        predictions=pred_labels,\n",
    "        references=labels,\n",
    "        num_labels=len(id2label),\n",
    "        ignore_index=0,\n",
    "        reduce_labels=processor.do_reduce_labels,\n",
    "    )\n",
    "    \n",
    "    # remove not necessary metrics\n",
    "    metrics.pop(\"per_category_accuracy\")\n",
    "    metrics.pop(\"mean_accuracy\")\n",
    "    metrics.pop(\"overall_accuracy\")\n",
    "    metrics.pop(\"per_category_iou\")    \n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the trainer for trainning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon ERROR @ 03:43:03] Error: Another instance of codecarbon is probably running as we find `/tmp/.codecarbon.lock`. Turn off the other instance to be able to run this one or use `allow_multiple_runs` or delete the file. Exiting.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch the trainning : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./segformer_wide_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of all models\n",
    "\n",
    "Here we evaluate each of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The vanilla model (just the segformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints_path = \"./training_logs\"\n",
    "epochs = 80\n",
    "lr = 0.00006\n",
    "batch_size = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_name = \"./segformer_vanilla\" \n",
    "model_eval = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    pretrained_model_name,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args_eval = TrainingArguments(\n",
    "    checkpoints_path,\n",
    "    learning_rate=lr,\n",
    "    num_train_epochs=epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    save_total_limit=3,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=20,\n",
    "    eval_steps=20,\n",
    "    logging_steps=1,\n",
    "    eval_accumulation_steps=5,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_mean_iou\",\n",
    "    greater_is_better = True,\n",
    "    lr_scheduler_type=\"polynomial\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon ERROR @ 03:43:05] Error: Another instance of codecarbon is probably running as we find `/tmp/.codecarbon.lock`. Turn off the other instance to be able to run this one or use `allow_multiple_runs` or delete the file. Exiting.\n"
     ]
    }
   ],
   "source": [
    "trainer_eval = Trainer(\n",
    "    model=model_eval,\n",
    "    args=training_args_eval,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model segformer_vanilla:\n",
      "    total number of parameters :27373539\n",
      "    number of parameters on the decoder :3177251\n",
      "\n",
      " Decoder architecture:\n",
      "SegformerDecodeHead(\n",
      "  (linear_c): ModuleList(\n",
      "    (0): SegformerMLP(\n",
      "      (proj): Linear(in_features=64, out_features=768, bias=True)\n",
      "    )\n",
      "    (1): SegformerMLP(\n",
      "      (proj): Linear(in_features=128, out_features=768, bias=True)\n",
      "    )\n",
      "    (2): SegformerMLP(\n",
      "      (proj): Linear(in_features=320, out_features=768, bias=True)\n",
      "    )\n",
      "    (3): SegformerMLP(\n",
      "      (proj): Linear(in_features=512, out_features=768, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (linear_fuse): Conv2d(3072, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (batch_norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (activation): ReLU()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Conv2d(768, 35, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n",
      "\n",
      " ### Evaluation of the model segformer_vanilla:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:14]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/datasets/features/image.py:348: UserWarning: Downcasting array dtype int64 to int32 to be compatible with 'Pillow'\n",
      "  warnings.warn(f\"Downcasting array dtype {dtype} to {dest_dtype} to be compatible with 'Pillow'\")\n",
      "/root/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--mean_iou/9e450724f21f05592bfb0255fe2fa576df8171fa060d11121d8aecfff0db80d0/mean_iou.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  iou = total_area_intersect / total_area_union\n",
      "/root/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--mean_iou/9e450724f21f05592bfb0255fe2fa576df8171fa060d11121d8aecfff0db80d0/mean_iou.py:260: RuntimeWarning: invalid value encountered in divide\n",
      "  acc = total_area_intersect / total_area_label\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.2091539353132248,\n",
       " 'eval_model_preparation_time': 0.0047,\n",
       " 'eval_mean_iou': 0.5765912893236104,\n",
       " 'eval_runtime': 19.8525,\n",
       " 'eval_samples_per_second': 10.074,\n",
       " 'eval_steps_per_second': 2.519}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Model \" + pretrained_model_name[2:] + \":\")\n",
    "print(f\"    total number of parameters :{model_eval.num_parameters()}\")\n",
    "print(f\"    number of parameters on the decoder :{model_eval.decode_head.num_parameters()}\")\n",
    "print(\"\\n Decoder architecture:\")\n",
    "print(model_eval.decode_head)\n",
    "print(\"\\n ### Evaluation of the model \" + pretrained_model_name[2:] + \":\" )\n",
    "trainer_eval.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here we test all the long decoder\n",
    "\n",
    "First the segformer_long_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon ERROR @ 03:43:25] Error: Another instance of codecarbon is probably running as we find `/tmp/.codecarbon.lock`. Turn off the other instance to be able to run this one or use `allow_multiple_runs` or delete the file. Exiting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model segformer_long_3:\n",
      "    total number of parameters :29149923\n",
      "    number of parameters on the decoder :4953635\n",
      "\n",
      " Decoder architecture:\n",
      "SegformerDecodeHead(\n",
      "  (linear_c): ModuleList(\n",
      "    (0): SegformerMLP(\n",
      "      (proj): Linear(in_features=64, out_features=768, bias=True)\n",
      "    )\n",
      "    (1): SegformerMLP(\n",
      "      (proj): Linear(in_features=128, out_features=768, bias=True)\n",
      "    )\n",
      "    (2): SegformerMLP(\n",
      "      (proj): Linear(in_features=320, out_features=768, bias=True)\n",
      "    )\n",
      "    (3): SegformerMLP(\n",
      "      (proj): Linear(in_features=512, out_features=768, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (linear_fuse): Conv2d(3072, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (batch_norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (activation): ReLU()\n",
      "  (long_segment): Sequential(\n",
      "    (0): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.1, inplace=False)\n",
      "    (4): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (5): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): Dropout(p=0.1, inplace=False)\n",
      "    (8): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (9): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU()\n",
      "    (11): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Conv2d(768, 35, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n",
      "\n",
      " ### Evaluation of the model segformer_long_3:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:14]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.15932905673980713,\n",
       " 'eval_model_preparation_time': 0.0042,\n",
       " 'eval_mean_iou': 0.7001308792788314,\n",
       " 'eval_runtime': 19.708,\n",
       " 'eval_samples_per_second': 10.148,\n",
       " 'eval_steps_per_second': 2.537}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model_name = \"./segformer_long_3\" \n",
    "model_eval = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    pretrained_model_name,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "trainer_eval = Trainer(\n",
    "    model=model_eval,\n",
    "    args=training_args_eval,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "print(\"Model \" + pretrained_model_name[2:] + \":\")\n",
    "print(f\"    total number of parameters :{model_eval.num_parameters()}\")\n",
    "print(f\"    number of parameters on the decoder :{model_eval.decode_head.num_parameters()}\")\n",
    "print(\"\\n Decoder architecture:\")\n",
    "print(model_eval.decode_head)\n",
    "print(\"\\n ### Evaluation of the model \" + pretrained_model_name[2:] + \":\" )\n",
    "trainer_eval.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the segformer_long_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon ERROR @ 03:43:45] Error: Another instance of codecarbon is probably running as we find `/tmp/.codecarbon.lock`. Turn off the other instance to be able to run this one or use `allow_multiple_runs` or delete the file. Exiting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model segformer_long_5:\n",
      "    total number of parameters :30334179\n",
      "    number of parameters on the decoder :6137891\n",
      "\n",
      " Decoder architecture:\n",
      "SegformerDecodeHead(\n",
      "  (linear_c): ModuleList(\n",
      "    (0): SegformerMLP(\n",
      "      (proj): Linear(in_features=64, out_features=768, bias=True)\n",
      "    )\n",
      "    (1): SegformerMLP(\n",
      "      (proj): Linear(in_features=128, out_features=768, bias=True)\n",
      "    )\n",
      "    (2): SegformerMLP(\n",
      "      (proj): Linear(in_features=320, out_features=768, bias=True)\n",
      "    )\n",
      "    (3): SegformerMLP(\n",
      "      (proj): Linear(in_features=512, out_features=768, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (linear_fuse): Conv2d(3072, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (batch_norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (activation): ReLU()\n",
      "  (long_segment): Sequential(\n",
      "    (0): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.1, inplace=False)\n",
      "    (4): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (5): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): Dropout(p=0.1, inplace=False)\n",
      "    (8): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (9): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU()\n",
      "    (11): Dropout(p=0.1, inplace=False)\n",
      "    (12): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (13): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (14): ReLU()\n",
      "    (15): Dropout(p=0.1, inplace=False)\n",
      "    (16): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (17): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (18): ReLU()\n",
      "    (19): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Conv2d(768, 35, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n",
      "\n",
      " ### Evaluation of the model segformer_long_5:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.1676626354455948,\n",
       " 'eval_model_preparation_time': 0.0043,\n",
       " 'eval_mean_iou': 0.7037701836403736,\n",
       " 'eval_runtime': 20.4524,\n",
       " 'eval_samples_per_second': 9.779,\n",
       " 'eval_steps_per_second': 2.445}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model_name = \"./segformer_long_5\" \n",
    "model_eval = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    pretrained_model_name,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "trainer_eval = Trainer(\n",
    "    model=model_eval,\n",
    "    args=training_args_eval,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "print(\"Model \" + pretrained_model_name[2:] + \":\")\n",
    "print(f\"    total number of parameters :{model_eval.num_parameters()}\")\n",
    "print(f\"    number of parameters on the decoder :{model_eval.decode_head.num_parameters()}\")\n",
    "print(\"\\n Decoder architecture:\")\n",
    "print(model_eval.decode_head)\n",
    "print(\"\\n ### Evaluation of the model \" + pretrained_model_name[2:] + \":\" )\n",
    "trainer_eval.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally the segformer_long_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon ERROR @ 03:44:06] Error: Another instance of codecarbon is probably running as we find `/tmp/.codecarbon.lock`. Turn off the other instance to be able to run this one or use `allow_multiple_runs` or delete the file. Exiting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model segformer_long_8:\n",
      "    total number of parameters :32110563\n",
      "    number of parameters on the decoder :7914275\n",
      "\n",
      " Decoder architecture:\n",
      "SegformerDecodeHead(\n",
      "  (linear_c): ModuleList(\n",
      "    (0): SegformerMLP(\n",
      "      (proj): Linear(in_features=64, out_features=768, bias=True)\n",
      "    )\n",
      "    (1): SegformerMLP(\n",
      "      (proj): Linear(in_features=128, out_features=768, bias=True)\n",
      "    )\n",
      "    (2): SegformerMLP(\n",
      "      (proj): Linear(in_features=320, out_features=768, bias=True)\n",
      "    )\n",
      "    (3): SegformerMLP(\n",
      "      (proj): Linear(in_features=512, out_features=768, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (linear_fuse): Conv2d(3072, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (batch_norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (activation): ReLU()\n",
      "  (long_segment): Sequential(\n",
      "    (0): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.1, inplace=False)\n",
      "    (4): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (5): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): Dropout(p=0.1, inplace=False)\n",
      "    (8): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (9): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU()\n",
      "    (11): Dropout(p=0.1, inplace=False)\n",
      "    (12): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (13): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (14): ReLU()\n",
      "    (15): Dropout(p=0.1, inplace=False)\n",
      "    (16): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (17): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (18): ReLU()\n",
      "    (19): Dropout(p=0.1, inplace=False)\n",
      "    (20): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (21): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (22): ReLU()\n",
      "    (23): Dropout(p=0.1, inplace=False)\n",
      "    (24): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (25): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (26): ReLU()\n",
      "    (27): Dropout(p=0.1, inplace=False)\n",
      "    (28): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (29): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (30): ReLU()\n",
      "    (31): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Conv2d(768, 35, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n",
      "\n",
      " ### Evaluation of the model segformer_long_8:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:16]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.18361562490463257,\n",
       " 'eval_model_preparation_time': 0.0046,\n",
       " 'eval_mean_iou': 0.7046008187441395,\n",
       " 'eval_runtime': 21.9321,\n",
       " 'eval_samples_per_second': 9.119,\n",
       " 'eval_steps_per_second': 2.28}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model_name = \"./segformer_long_8\" \n",
    "model_eval = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    pretrained_model_name,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "trainer_eval = Trainer(\n",
    "    model=model_eval,\n",
    "    args=training_args_eval,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "print(\"Model \" + pretrained_model_name[2:] + \":\")\n",
    "print(f\"    total number of parameters :{model_eval.num_parameters()}\")\n",
    "print(f\"    number of parameters on the decoder :{model_eval.decode_head.num_parameters()}\")\n",
    "print(\"\\n Decoder architecture:\")\n",
    "print(model_eval.decode_head)\n",
    "print(\"\\n ### Evaluation of the model \" + pretrained_model_name[2:] + \":\" )\n",
    "trainer_eval.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here we test all the wide decoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then segformer_wide_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon ERROR @ 03:44:28] Error: Another instance of codecarbon is probably running as we find `/tmp/.codecarbon.lock`. Turn off the other instance to be able to run this one or use `allow_multiple_runs` or delete the file. Exiting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model segformer_wide_3:\n",
      "    total number of parameters :31475171\n",
      "    number of parameters on the decoder :7278883\n",
      "\n",
      " Decoder architecture:\n",
      "SegformerDecodeHead(\n",
      "  (linear_c): ModuleList(\n",
      "    (0): SegformerMLP(\n",
      "      (proj): Linear(in_features=64, out_features=768, bias=True)\n",
      "    )\n",
      "    (1): SegformerMLP(\n",
      "      (proj): Linear(in_features=128, out_features=768, bias=True)\n",
      "    )\n",
      "    (2): SegformerMLP(\n",
      "      (proj): Linear(in_features=320, out_features=768, bias=True)\n",
      "    )\n",
      "    (3): SegformerMLP(\n",
      "      (proj): Linear(in_features=512, out_features=768, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (linear_fuse): Conv2d(3072, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (batch_norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (activation): ReLU()\n",
      "  (wide_segment): Sequential(\n",
      "    (0): Conv2d(768, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.1, inplace=False)\n",
      "    (4): Conv2d(1024, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (5): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): Dropout(p=0.1, inplace=False)\n",
      "    (8): Conv2d(1280, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (9): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU()\n",
      "    (11): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Conv2d(1536, 35, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n",
      "\n",
      " ### Evaluation of the model segformer_wide_3:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.12735484540462494,\n",
       " 'eval_model_preparation_time': 0.0042,\n",
       " 'eval_mean_iou': 0.766613073891128,\n",
       " 'eval_runtime': 21.2249,\n",
       " 'eval_samples_per_second': 9.423,\n",
       " 'eval_steps_per_second': 2.356}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model_name = \"./segformer_wide_3\" \n",
    "model_eval = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    pretrained_model_name,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "trainer_eval = Trainer(\n",
    "    model=model_eval,\n",
    "    args=training_args_eval,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "print(\"Model \" + pretrained_model_name[2:] + \":\")\n",
    "print(f\"    total number of parameters :{model_eval.num_parameters()}\")\n",
    "print(f\"    number of parameters on the decoder :{model_eval.decode_head.num_parameters()}\")\n",
    "print(\"\\n Decoder architecture:\")\n",
    "print(model_eval.decode_head)\n",
    "print(\"\\n ### Evaluation of the model \" + pretrained_model_name[2:] + \":\" )\n",
    "trainer_eval.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally segformer_wide_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon ERROR @ 03:44:49] Error: Another instance of codecarbon is probably running as we find `/tmp/.codecarbon.lock`. Turn off the other instance to be able to run this one or use `allow_multiple_runs` or delete the file. Exiting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model segformer_wide_4:\n",
      "    total number of parameters :34242019\n",
      "    number of parameters on the decoder :10045731\n",
      "\n",
      " Decoder architecture:\n",
      "SegformerDecodeHead(\n",
      "  (linear_c): ModuleList(\n",
      "    (0): SegformerMLP(\n",
      "      (proj): Linear(in_features=64, out_features=768, bias=True)\n",
      "    )\n",
      "    (1): SegformerMLP(\n",
      "      (proj): Linear(in_features=128, out_features=768, bias=True)\n",
      "    )\n",
      "    (2): SegformerMLP(\n",
      "      (proj): Linear(in_features=320, out_features=768, bias=True)\n",
      "    )\n",
      "    (3): SegformerMLP(\n",
      "      (proj): Linear(in_features=512, out_features=768, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (linear_fuse): Conv2d(3072, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (batch_norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (activation): ReLU()\n",
      "  (wide_segment): Sequential(\n",
      "    (0): Conv2d(768, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.1, inplace=False)\n",
      "    (4): Conv2d(1024, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (5): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): Dropout(p=0.1, inplace=False)\n",
      "    (8): Conv2d(1280, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (9): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU()\n",
      "    (11): Dropout(p=0.1, inplace=False)\n",
      "    (12): Conv2d(1536, 1792, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (13): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (14): ReLU()\n",
      "    (15): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Conv2d(1792, 35, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n",
      "\n",
      " ### Evaluation of the model segformer_wide_4:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:17]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.1819719523191452,\n",
       " 'eval_model_preparation_time': 0.0043,\n",
       " 'eval_mean_iou': 0.6951090629663104,\n",
       " 'eval_runtime': 22.9251,\n",
       " 'eval_samples_per_second': 8.724,\n",
       " 'eval_steps_per_second': 2.181}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model_name = \"./segformer_wide_4\" \n",
    "model_eval = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    pretrained_model_name,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "trainer_eval = Trainer(\n",
    "    model=model_eval,\n",
    "    args=training_args_eval,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "print(\"Model \" + pretrained_model_name[2:] + \":\")\n",
    "print(f\"    total number of parameters :{model_eval.num_parameters()}\")\n",
    "print(f\"    number of parameters on the decoder :{model_eval.decode_head.num_parameters()}\")\n",
    "print(\"\\n Decoder architecture:\")\n",
    "print(model_eval.decode_head)\n",
    "print(\"\\n ### Evaluation of the model \" + pretrained_model_name[2:] + \":\" )\n",
    "trainer_eval.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
