{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Segformer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "from huggingface_hub import hf_hub_download\n",
    "import json\n",
    "import torchvision.transforms as t\n",
    "from transformers import AutoImageProcessor\n",
    "from torchvision.transforms import ColorJitter\n",
    "from transformers import SegformerImageProcessor\n",
    "from torchvision.transforms import v2\n",
    "import torch\n",
    "from torch import nn\n",
    "import evaluate\n",
    "from transformers import Trainer\n",
    "from transformers import SegformerForSemanticSegmentation\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access token to get access to the dataset. \n",
    "\n",
    "You just need a HuggingFace account to have access to it. \n",
    "\n",
    "If you don't have a HuggingFace account use the temporary token shared in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_token = \"place_token_in_this string\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_dataset_identifier = \"segments/sidewalk-semantic\"\n",
    "ds = load_dataset(hf_dataset_identifier,\n",
    "                   token=access_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare labels :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"id2label.json\"\n",
    "id2label = json.load(\n",
    "    open(hf_hub_download(hf_dataset_identifier, filename, repo_type=\"dataset\",token=access_token), \"r\")\n",
    ")\n",
    "id2label = {int(k): v for k, v in id2label.items()}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "num_labels = len(id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels, list(label2id.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the preprocessing for the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_name = \"nvidia/mit-b2\" \n",
    "image_processor = AutoImageProcessor.from_pretrained(pretrained_model_name)\n",
    "image_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def transforms(image):\n",
    "    image = t.ToTensor(image)\n",
    "    image = image.permute(\n",
    "        (2, 0, 1)\n",
    "    )  # because model is channels-first\n",
    "    return image\n",
    "\n",
    "\n",
    "def preprocess(example_batch):\n",
    "    images = [transforms(x.convert(\"RGB\")) for x in example_batch[\"pixel_values\"]]\n",
    "    labels = [x for x in example_batch[\"label\"]]\n",
    "    inputs = image_processor(images, labels)\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "processor = SegformerImageProcessor()\n",
    "\n",
    "image_only_transforms = v2.Compose([\n",
    "     ColorJitter(brightness=0.25, contrast=0.25, saturation=0.25, hue=0.1),\n",
    "])\n",
    "label_image_transforms = v2.Compose([\n",
    "    v2.RandomHorizontalFlip(p=0.5),\n",
    "    v2.RandomVerticalFlip(p=0.5),\n",
    "])\n",
    "\n",
    "def train_transforms(example_batch):\n",
    "    images = [image_only_transforms(x) for x in example_batch['pixel_values']]\n",
    "    labels = [x for x in example_batch['label']]\n",
    "\n",
    "    images, labels= label_image_transforms(images, labels)\n",
    "    inputs = processor(images, labels)\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def val_transforms(example_batch):\n",
    "    images = [x for x in example_batch['pixel_values']]\n",
    "    labels = [x for x in example_batch['label']]\n",
    "    inputs = processor(images, labels)\n",
    "    return inputs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split up training into training + validation\n",
    "splits = ds[\"train\"].train_test_split(test_size=0.2)\n",
    "train_ds = splits[\"train\"]\n",
    "val_ds = splits[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set transforms\n",
    "train_ds.set_transform(train_transforms)\n",
    "val_ds.set_transform(val_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load our segformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#use the mit-b2 encoder for our experiments\n",
    "pretrained_model_name = \"nvidia/mit-b2\" \n",
    "model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    pretrained_model_name,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    long_decoder_depth=0,\n",
    "    width_list=[1024, 1280, 1536, 1792]\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.decode_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.decode_head.num_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set the parameters for the trainning pipeline :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#set the parameters for the trainning \n",
    "epochs = 80\n",
    "lr = 0.00006\n",
    "batch_size = 4\n",
    "checkpoints_path = \"./training_logs\"\n",
    "training_args = TrainingArguments(\n",
    "    checkpoints_path,\n",
    "    learning_rate=lr,\n",
    "    num_train_epochs=epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    save_total_limit=3,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=20,\n",
    "    eval_steps=20,\n",
    "    logging_steps=1,\n",
    "    eval_accumulation_steps=5,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_mean_iou\",\n",
    "    greater_is_better = True,\n",
    "    lr_scheduler_type=\"polynomial\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metric for the trainning and the evaluation (loss and mIOU):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "metric = evaluate.load(\"mean_iou\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "  with torch.no_grad():\n",
    "    logits, labels = eval_pred\n",
    "    logits_tensor = torch.from_numpy(logits)\n",
    "    # scale the logits to the size of the label\n",
    "    logits_tensor = nn.functional.interpolate(\n",
    "        logits_tensor,\n",
    "        size=labels.shape[-2:],\n",
    "        mode=\"bilinear\",\n",
    "        align_corners=False,\n",
    "    ).argmax(dim=1)\n",
    "\n",
    "    pred_labels = logits_tensor.detach().cpu().numpy()\n",
    "    metrics = metric.compute(\n",
    "        predictions=pred_labels,\n",
    "        references=labels,\n",
    "        num_labels=len(id2label),\n",
    "        ignore_index=0,\n",
    "        reduce_labels=processor.do_reduce_labels,\n",
    "    )\n",
    "    \n",
    "    # remove not necessary metrics\n",
    "    metrics.pop(\"per_category_accuracy\")\n",
    "    metrics.pop(\"mean_accuracy\")\n",
    "    metrics.pop(\"overall_accuracy\")\n",
    "    metrics.pop(\"per_category_iou\")    \n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the trainer for trainning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch the trainning : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./segformer_wide_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of all models\n",
    "\n",
    "Here we evaluate each of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The vanilla model (just the segformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints_path = \"./training_logs\"\n",
    "epochs = 80\n",
    "lr = 0.00006\n",
    "batch_size = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_name = \"./segformer_vanilla\" \n",
    "model_eval = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    pretrained_model_name,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args_eval = TrainingArguments(\n",
    "    checkpoints_path,\n",
    "    learning_rate=lr,\n",
    "    num_train_epochs=epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    save_total_limit=3,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=20,\n",
    "    eval_steps=20,\n",
    "    logging_steps=1,\n",
    "    eval_accumulation_steps=5,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_mean_iou\",\n",
    "    greater_is_better = True,\n",
    "    lr_scheduler_type=\"polynomial\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_eval = Trainer(\n",
    "    model=model_eval,\n",
    "    args=training_args_eval,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model \" + pretrained_model_name[2:] + \":\")\n",
    "print(f\"    total number of parameters :{model_eval.num_parameters()}\")\n",
    "print(f\"    number of parameters on the decoder :{model_eval.decode_head.num_parameters()}\")\n",
    "print(\"\\n Decoder architecture:\")\n",
    "print(model_eval.decode_head)\n",
    "print(\"\\n ### Evaluation of the model \" + pretrained_model_name[2:] + \":\" )\n",
    "trainer_eval.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here we test all the long decoder\n",
    "\n",
    "First the segformer_long_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_name = \"./segformer_long_3\" \n",
    "model_eval = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    pretrained_model_name,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "trainer_eval = Trainer(\n",
    "    model=model_eval,\n",
    "    args=training_args_eval,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "print(\"Model \" + pretrained_model_name[2:] + \":\")\n",
    "print(f\"    total number of parameters :{model_eval.num_parameters()}\")\n",
    "print(f\"    number of parameters on the decoder :{model_eval.decode_head.num_parameters()}\")\n",
    "print(\"\\n Decoder architecture:\")\n",
    "print(model_eval.decode_head)\n",
    "print(\"\\n ### Evaluation of the model \" + pretrained_model_name[2:] + \":\" )\n",
    "trainer_eval.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the segformer_long_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_name = \"./segformer_long_5\" \n",
    "model_eval = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    pretrained_model_name,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "trainer_eval = Trainer(\n",
    "    model=model_eval,\n",
    "    args=training_args_eval,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "print(\"Model \" + pretrained_model_name[2:] + \":\")\n",
    "print(f\"    total number of parameters :{model_eval.num_parameters()}\")\n",
    "print(f\"    number of parameters on the decoder :{model_eval.decode_head.num_parameters()}\")\n",
    "print(\"\\n Decoder architecture:\")\n",
    "print(model_eval.decode_head)\n",
    "print(\"\\n ### Evaluation of the model \" + pretrained_model_name[2:] + \":\" )\n",
    "trainer_eval.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally the segformer_long_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_name = \"./segformer_long_8\" \n",
    "model_eval = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    pretrained_model_name,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "trainer_eval = Trainer(\n",
    "    model=model_eval,\n",
    "    args=training_args_eval,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "print(\"Model \" + pretrained_model_name[2:] + \":\")\n",
    "print(f\"    total number of parameters :{model_eval.num_parameters()}\")\n",
    "print(f\"    number of parameters on the decoder :{model_eval.decode_head.num_parameters()}\")\n",
    "print(\"\\n Decoder architecture:\")\n",
    "print(model_eval.decode_head)\n",
    "print(\"\\n ### Evaluation of the model \" + pretrained_model_name[2:] + \":\" )\n",
    "trainer_eval.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here we test all the wide decoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then segformer_wide_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_name = \"./segformer_wide_3\" \n",
    "model_eval = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    pretrained_model_name,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "trainer_eval = Trainer(\n",
    "    model=model_eval,\n",
    "    args=training_args_eval,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "print(\"Model \" + pretrained_model_name[2:] + \":\")\n",
    "print(f\"    total number of parameters :{model_eval.num_parameters()}\")\n",
    "print(f\"    number of parameters on the decoder :{model_eval.decode_head.num_parameters()}\")\n",
    "print(\"\\n Decoder architecture:\")\n",
    "print(model_eval.decode_head)\n",
    "print(\"\\n ### Evaluation of the model \" + pretrained_model_name[2:] + \":\" )\n",
    "trainer_eval.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally segformer_wide_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_name = \"./segformer_wide_4\" \n",
    "model_eval = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    pretrained_model_name,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "trainer_eval = Trainer(\n",
    "    model=model_eval,\n",
    "    args=training_args_eval,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "print(\"Model \" + pretrained_model_name[2:] + \":\")\n",
    "print(f\"    total number of parameters :{model_eval.num_parameters()}\")\n",
    "print(f\"    number of parameters on the decoder :{model_eval.decode_head.num_parameters()}\")\n",
    "print(\"\\n Decoder architecture:\")\n",
    "print(model_eval.decode_head)\n",
    "print(\"\\n ### Evaluation of the model \" + pretrained_model_name[2:] + \":\" )\n",
    "trainer_eval.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
